# IRWA_Project 

**Part2**:

- The main function of the code is "evaluation", this function is the one in charge of processing the queries, both the baseline queries provided and the custom queries made by us. This function essentially creates a small index with the corresponding subset of documents (tweets) that belongs to each query test and then it computes the prior evaluation metrics with functions defined for every metric. 
To execute the evaluation of the baseline and custom queries it is only required to execute the “main()” function in “index.py” and it will print the metrics of evaluation for each query and then it will plot the 2D-scatter t-SNE of the word2vec representation of the tweets.
 

- In evaluate_query.py, we have a functionality where the user can query our database of tweets and the system will write the ranked tweets in a txt file for the user to review. Essentially, it first creates the index for the entire tweets database, and has a loop where the user can provide queries.

**Part3**:

- Ranking Scores: Once you execute the 'execute_query.py', the program will take a few minutes creating the index and the embeddings we created for the tweets. After that, it will ask the user for a query, which will be used to rank the tweets according to two different ranking methods. The first one is the traditional tf-idf score with cosine similarity, as already implemented in the last part. The second score, from our choice, consists on a function of the cosine similarity of the tweets and query embeddings and the likes and retweets of the tweets. Thus, two files will be created to store the results: ranking_tfidf.txt will store the top 20 tweets according the tf-idf score, and ranking_custom.txt will store the top 20 tweets according our custom score. The k value is set by default at 20, since it is what the statement asked for, but it can be easily changed in the code.

- Word2vec: The implementation of our 'Word2Vec' ranking, or rather 'Tweet2Vec' works as following. We first compute a data structure that stores the tweet vector for each tweetId, in that way, we don't need to recompute the vectors for each query. Secondly, there is a loop that asks the user for an input. For each given input, we obtain the terms of the query that are in our index, and retrieve only those documents that contain all the terms. Subsequently, we retrieve a subset of our vector data structure to retrieve only those tweets that matched all terms in the query, and perform cosine similarity only with that subset. Finally, we write the top 20 results in a text file. Between all of our 5 queries, we only got matching tweets for the first one, since it was the only one where all the query terms - present in our index- were also in the tweet that was retrieved. This makes sense because in our case, our queries are a bit large for this rather simple model of retrieving tweets. When a query is too long, it gets increasingly more rare that the model will return any matching query. That is because of what I just mentioned, which is that we only retrieve tweets that have all the terms of the query -present in the index-.
